name: SEO Agent (auto-build -> check -> ping)

on:
  push:
    branches:
      - main
      - master
  schedule:
    - cron: "0 0 * * *"   # 매일 00:00 UTC (KST 09:00)
  workflow_dispatch:

permissions:
  contents: write

concurrency:
  group: seo-agent
  cancel-in-progress: false

jobs:
  guard:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    env:
      SITE_BASE: ${{ secrets.SITE_BASE }}
      SITEMAP_PATH: ${{ secrets.SITEMAP_PATH }}
      DEPLOY_HOOK_URL: ${{ secrets.DEPLOY_HOOK_URL }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Validate secrets
        shell: bash
        run: |
          set -e
          : "${SITE_BASE:?Set SITE_BASE in repo secrets}"
          echo "SITEMAP_PATH=${SITEMAP_PATH:-/sitemap.xml}" >> $GITHUB_ENV

      - name: Auto-build sitemap.xml, sitemap_index.xml & robots.txt
        shell: bash
        env:
          BRANCH: ${{ github.ref_name }}
        run: |
          set -e
          BASE="${SITE_BASE%/}"
          TODAY="$(date -u +%F)"

          # 원격 최신으로 동기화(푸시 거절 방지)
          git fetch origin "$BRANCH"
          git checkout "$BRANCH"
          git reset --hard "origin/$BRANCH"

          # 공개 HTML 수집(시스템 폴더 제외)
          mapfile -t FILES < <(find . -type f -name '*.html' \
            -not -path './.git/*' -not -path './.github/*' -not -path './node_modules/*' | sort)

          # 제외: status.html, google*.html
          PAGES=""
          for f in "${FILES[@]}"; do
            base="$(basename "$f")"
            case "$base" in
              status.html|google*.html) continue ;;
            esac
            PAGES+="$f"$'\n'
          done

          # sitemap.xml 생성
          {
            echo '<?xml version="1.0" encoding="UTF-8"?>'
            echo '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">'
            IFS=$'\n'
            for f in $PAGES; do
              rel="${f#./}"
              dir="$(dirname "$rel")"
              base="$(basename "$rel")"

              if [ "$base" = "index.html" ]; then
                if [ "$dir" = "." ] || [ "$dir" = "/" ]; then
                  loc="${BASE}/"
                else
                  loc="${BASE}/${dir}/"
                fi
                cf=weekly; pr=1.0
              else
                loc="${BASE}/${rel}"
                cf=monthly; pr=0.8
              fi

              echo '  <url>'
              echo "    <loc>$loc</loc>"
              echo "    <lastmod>${TODAY}</lastmod>"
              echo "    <changefreq>$cf</changefreq>"
              echo "    <priority>$pr</priority>"
              echo '  </url>'
            done
            echo '</urlset>'
          } > sitemap.xml

          # robots.txt 생성/갱신 (인덱스 포함)
          {
            echo 'User-agent: *'
            echo 'Allow: /'
            echo
            echo "Sitemap: $BASE/sitemap_index.xml"
            echo "Sitemap: $BASE/sitemap.xml"
          } > robots.txt

          # sitemap_index.xml 생성
          {
            echo '<?xml version="1.0" encoding="UTF-8"?>'
            echo '<sitemapindex xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">'
            echo '  <sitemap>'
            echo "    <loc>${BASE}/sitemap.xml</loc>"
            echo "    <lastmod>${TODAY}</lastmod>"
            echo '  </sitemap>'
            echo '</sitemapindex>'
          } > sitemap_index.xml

          # 자동 커밋/푸시 ([skip ci]로 루프 방지)
          git add sitemap.xml sitemap_index.xml robots.txt
          if ! git diff --cached --quiet; then
            git -c user.name='seo-agent' -c user.email='seo-agent@users.noreply.github.com' \
              commit -m "[seo-agent] Update sitemap(s)/robots [skip ci]"
            git push origin "HEAD:$BRANCH"
          else
            echo "No changes to sitemap/robots."
          fi

      - name: Check headers (status & content-type)
        shell: bash
        run: |
          URL="${SITE_BASE}${SITEMAP_PATH:-/sitemap.xml}"
          HEADERS=$(curl -sSIL "$URL")
          echo "$HEADERS" > /tmp/headers.txt

          STATUS=$(awk '/^HTTP/{code=$2} END{print code}' /tmp/headers.txt)
          CT=$(awk 'tolower($0) ~ /^content-type:/{line=$0} END{print line}' /tmp/headers.txt | awk -F': ' '{print tolower($2)}')

          echo "status=$STATUS"
          echo "content-type=$CT"

          case "$STATUS" in
            2*) : ;;
            *) echo "Non-2xx status: $STATUS"; exit 2 ;;
          esac

          if echo "$CT" | grep -qi 'application/x-gzip'; then
            echo "IS_GZ=true" >> $GITHUB_ENV
          elif echo "$CT" | grep -Eqi 'application/xml|text/xml'; then
            echo "IS_GZ=false" >> $GITHUB_ENV
          else
            echo "Unexpected Content-Type: $CT"; exit 3
          fi

      - name: Ensure body has no <script> (non-gzip)
        if: env.IS_GZ != 'true'
        shell: bash
        run: |
          URL="${SITE_BASE}${SITEMAP_PATH:-/sitemap.xml}"
          BODY=$(curl -sS "$URL")
          if echo "$BODY" | grep -qi '<script'; then
            echo 'Found <script> in sitemap'; exit 4
          fi

      - name: Ping Google
        shell: bash
        run: |
          URL="${SITE_BASE}${SITEMAP_PATH:-/sitemap.xml}"
          ENCODED=$(python3 - <<'PY'
import urllib.parse, os
print(urllib.parse.quote(os.environ["URL"], safe=""))
PY
)
          PING="https://www.google.com/ping?sitemap=${ENCODED}"
          echo "Ping => $PING"
          curl -sS "$PING" >/dev/null || true

      - name: Redeploy via hook (on failure)
        if: "failure() && env.DEPLOY_HOOK_URL != ''"
        shell: bash
        run: |
          curl -sS -X POST "$DEPLOY_HOOK_URL" || true
